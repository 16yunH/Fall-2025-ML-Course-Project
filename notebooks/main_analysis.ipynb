{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "77b38235",
   "metadata": {},
   "source": [
    "# Dairy Cow Milk Yield Prediction - Complete ML Pipeline\n",
    "\n",
    "**Course**: CS 363M Machine Learning  \n",
    "**Project**: Fall 2025 Kaggle Competition  \n",
    "**Task**: Predict milk yield for 250,000 dairy cows  \n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This notebook contains a complete machine learning workflow:\n",
    "1. **Data Loading & Exploration** - Understand data structure and distribution\n",
    "2. **Data Cleaning** - Handle missing values and outliers\n",
    "3. **Feature Engineering** - Create and transform features\n",
    "4. **Feature Selection** - Select most relevant features\n",
    "5. **Model Training** - Train multiple models\n",
    "6. **Model Evaluation** - Compare model performance\n",
    "7. **Ensemble Learning** - Use ensemble methods to improve performance\n",
    "8. **Prediction & Submission** - Generate Kaggle submission file\n",
    "\n",
    "**Grading Criteria**:\n",
    "- 10% Kaggle leaderboard score\n",
    "- 90% Jupyter Notebook methodology\n",
    "  - Data Cleaning (20%)\n",
    "  - Data Exploration (20%)\n",
    "  - Feature Engineering (20%)\n",
    "  - Modeling Approach (20%)\n",
    "  - Code Quality (20%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687844c0",
   "metadata": {},
   "source": [
    "## 0. Environment Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9216f6a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.ensemble import (RandomForestRegressor, GradientBoostingRegressor, \n",
    "                               AdaBoostRegressor, VotingRegressor, StackingRegressor)\n",
    "from scipy import stats\n",
    "from scipy.stats import norm, skew\n",
    "import joblib\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "DATA_DIR = Path('../data')\n",
    "RAW_DATA_DIR = DATA_DIR / 'raw'\n",
    "PROCESSED_DATA_DIR = DATA_DIR / 'processed'\n",
    "MODEL_DIR = Path('../models')\n",
    "SUBMISSION_DIR = Path('../submissions')\n",
    "\n",
    "for dir_path in [PROCESSED_DATA_DIR, MODEL_DIR, SUBMISSION_DIR]:\n",
    "    dir_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"Random state: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179389f9",
   "metadata": {},
   "source": [
    "## 1. Initial Data Exploration\n",
    "\n",
    "**Objective**: Load and understand the dataset structure\n",
    "\n",
    "In this section, we will:\n",
    "1. Load training and testing datasets\n",
    "2. View basic information about the data\n",
    "3. Check data types and missing values\n",
    "4. Perform preliminary statistical analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7da606a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(RAW_DATA_DIR / 'cattle_data_train.csv')\n",
    "test = pd.read_csv(RAW_DATA_DIR / 'cattle_data_test.csv')\n",
    "\n",
    "target_col = 'Milk_Yield_L'\n",
    "id_col = 'Cattle_ID'\n",
    "\n",
    "print(\"Data Loading Complete\")\n",
    "print(f\"Training set shape: {train.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")\n",
    "print(f\"Target variable: {target_col}\")\n",
    "print(f\"ID column: {id_col}\")\n",
    "print(f\"\\nColumn Validation:\")\n",
    "print(f\"  Target variable exists: {target_col in train.columns}\")\n",
    "print(f\"  ID column exists: {id_col in train.columns}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43dd711d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Set Information\")\n",
    "print(train.info())\n",
    "\n",
    "print(\"First 5 Rows\")\n",
    "print(train.head())\n",
    "\n",
    "print(\"Descriptive Statistics\")\n",
    "print(train.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af6f30e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = train.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "if target_col in numeric_features:\n",
    "    numeric_features.remove(target_col)\n",
    "if id_col in numeric_features:\n",
    "    numeric_features.remove(id_col)\n",
    "if id_col in categorical_features:\n",
    "    categorical_features.remove(id_col)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Feature Type Identification\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Number of numeric features: {len(numeric_features)}\")\n",
    "print(f\"Number of categorical features: {len(categorical_features)}\")\n",
    "print(f\"\\nNumeric features:\\n{numeric_features}\")\n",
    "print(f\"\\nCategorical features:\\n{categorical_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf24747",
   "metadata": {},
   "source": [
    "## Missing Value Analysis\n",
    "\n",
    "Missing values can impact model training. We need to:\n",
    "1. Calculate the number and percentage of missing values for each column\n",
    "2. Visualize the distribution of missing values\n",
    "3. Decide how to handle missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddc67a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_train = pd.DataFrame({\n",
    "    'column': train.columns,\n",
    "    'missing_count': train.isnull().sum(),\n",
    "    'missing_percent': (train.isnull().sum() / len(train) * 100).round(2)\n",
    "})\n",
    "missing_train = missing_train[missing_train['missing_count'] > 0].sort_values('missing_percent', ascending=False)\n",
    "\n",
    "if len(missing_train) > 0:\n",
    "    print(missing_train.to_string(index=False))\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.barh(missing_train['column'], missing_train['missing_percent'])\n",
    "    plt.xlabel('Missing Percentage (%)')\n",
    "    plt.title('Training Set Missing Values Distribution')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"No missing values in training set.\")\n",
    "\n",
    "missing_test = pd.DataFrame({\n",
    "    'column': test.columns,\n",
    "    'missing_count': test.isnull().sum(),\n",
    "    'missing_percent': (test.isnull().sum() / len(test) * 100).round(2)\n",
    "})\n",
    "missing_test = missing_test[missing_test['missing_count'] > 0].sort_values('missing_percent', ascending=False)\n",
    "\n",
    "print(\"Test set missing values summary\")\n",
    "if len(missing_test) > 0:\n",
    "    print(missing_test.to_string(index=False))\n",
    "else:\n",
    "    print(\"No missing values in test set.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a2cd163",
   "metadata": {},
   "source": [
    "## Target Variable Analysis\n",
    "\n",
    "Analyze distribution characteristics of the target variable (Milk Yield):\n",
    "- Distribution shape (normality test)\n",
    "- Skewness and kurtosis\n",
    "- Whether a transformation is needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6aaae82",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "axes[0, 0].hist(train[target_col], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Milk Yield (L)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Milk Yield Distribution - Histogram')\n",
    "axes[0, 0].axvline(train[target_col].mean(), color='red', linestyle='--', label=f'Mean: {train[target_col].mean():.2f}')\n",
    "axes[0, 0].axvline(train[target_col].median(), color='green', linestyle='--', label=f'Median: {train[target_col].median():.2f}')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "axes[0, 1].boxplot(train[target_col], vert=True)\n",
    "axes[0, 1].set_ylabel('Milk Yield (L)')\n",
    "axes[0, 1].set_title('Milk Yield Distribution - Boxplot')\n",
    "axes[0, 1].grid(axis='y', alpha=0.3)\n",
    "\n",
    "stats.probplot(train[target_col], dist=\"norm\", plot=axes[1, 0])\n",
    "axes[1, 0].set_title('Q-Q Plot (Normality Test)')\n",
    "\n",
    "train[target_col].plot(kind='density', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Milk Yield (L)')\n",
    "axes[1, 1].set_title('Milk Yield Distribution - Kernel Density Estimate')\n",
    "axes[1, 1].axvline(train[target_col].mean(), color='red', linestyle='--', label=f'Mean: {train[target_col].mean():.2f}')\n",
    "axes[1, 1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Target variable statistics\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Mean: {train[target_col].mean():.4f}\")\n",
    "print(f\"Median: {train[target_col].median():.4f}\")\n",
    "print(f\"Std: {train[target_col].std():.4f}\")\n",
    "print(f\"Skewness: {train[target_col].skew():.4f}\")\n",
    "print(f\"Kurtosis: {train[target_col].kurtosis():.4f}\")\n",
    "print(f\"Min: {train[target_col].min():.4f}\")\n",
    "print(f\"Max: {train[target_col].max():.4f}\")\n",
    "\n",
    "skewness = train[target_col].skew()\n",
    "if abs(skewness) < 0.5:\n",
    "    print(f\"\\nSkewness {skewness:.4f} - Distribution approximately symmetric\")\n",
    "elif abs(skewness) < 1:\n",
    "    print(f\"\\nSkewness {skewness:.4f} - Mildly skewed distribution\")\n",
    "else:\n",
    "    print(f\"\\nSkewness {skewness:.4f} - Highly skewed distribution; consider log transform\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ef64e30",
   "metadata": {},
   "source": [
    "## Numerical Feature Analysis\n",
    "\n",
    "Analyze the distributions and statistical properties of all numerical features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a294b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_features) > 0:\n",
    "    n_features = len(numeric_features)\n",
    "    n_cols = 3\n",
    "    n_rows = min(4, (n_features + n_cols - 1) // n_cols)\n",
    "    \n",
    "    for page in range((n_features + 11) // 12):\n",
    "        start_idx = page * 12\n",
    "        end_idx = min(start_idx + 12, n_features)\n",
    "        features_subset = numeric_features[start_idx:end_idx]\n",
    "        \n",
    "        n_plots = len(features_subset)\n",
    "        n_rows_actual = (n_plots + n_cols - 1) // n_cols\n",
    "        \n",
    "        fig, axes = plt.subplots(n_rows_actual, n_cols, figsize=(15, 4*n_rows_actual))\n",
    "        if isinstance(axes, np.ndarray):\n",
    "            axes = axes.flatten()\n",
    "        else:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for idx, feature in enumerate(features_subset):\n",
    "            axes[idx].hist(train[feature].dropna(), bins=30, edgecolor='black', alpha=0.7)\n",
    "            axes[idx].set_title(f'{feature}\\nSkewness: {train[feature].skew():.2f}')\n",
    "            axes[idx].set_xlabel(feature)\n",
    "            axes[idx].set_ylabel('Frequency')\n",
    "        \n",
    "        for idx in range(n_plots, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "        \n",
    "        plt.suptitle(f'Numerical Feature Distributions (Page {page+1})', y=1.00, fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eea06a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_features) > 0:\n",
    "    corr_features = numeric_features + [target_col]\n",
    "    correlation_matrix = train[corr_features].corr()\n",
    "    \n",
    "    plt.figure(figsize=(14, 12))\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    sns.heatmap(correlation_matrix, mask=mask, annot=False, cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=0.5, \n",
    "                cbar_kws={\"shrink\": 0.8})\n",
    "    plt.title('Feature Correlation Heatmap (Lower Triangle)', fontsize=14, pad=20)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    target_correlation = correlation_matrix[target_col].drop(target_col).sort_values(\n",
    "        key=abs, ascending=False\n",
    "    ).head(15)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    colors = ['green' if x > 0 else 'red' for x in target_correlation.values]\n",
    "    plt.barh(range(len(target_correlation)), target_correlation.values, color=colors)\n",
    "    plt.yticks(range(len(target_correlation)), target_correlation.index)\n",
    "    plt.xlabel('Correlation Coefficient')\n",
    "    plt.title('Top 15 Features Correlated with Target', fontsize=14)\n",
    "    plt.axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"Top 15 features most correlated with the target\")\n",
    "    print(\"=\" * 60)\n",
    "    print(target_correlation)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544c1357",
   "metadata": {},
   "source": [
    "## Categorical Feature Analysis\n",
    "\n",
    "Analysis of the distribution of categorical features and their relationship with the target variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65152dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categorical_features) > 0:\n",
    "    print(\"=\" * 60)\n",
    "    print(\"Categorical Feature Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        n_unique = train[feature].nunique()\n",
    "        print(f\"\\nFeature: {feature}\")\n",
    "        print(f\"  Number of Unique Values: {n_unique}\")\n",
    "        print(f\"  Number of Missing Values: {train[feature].isnull().sum()}\")\n",
    "        \n",
    "        if n_unique <= 10:\n",
    "            print(f\"  Value Distribution:\")\n",
    "            print(train[feature].value_counts().to_string())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d8e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(categorical_features) > 0:\n",
    "    for feature in categorical_features[:min(6, len(categorical_features))]:\n",
    "        n_unique = train[feature].nunique()\n",
    "        \n",
    "        if n_unique <= 20:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "            \n",
    "            value_counts = train[feature].value_counts()\n",
    "            axes[0].barh(range(len(value_counts)), value_counts.values)\n",
    "            axes[0].set_yticks(range(len(value_counts)))\n",
    "            axes[0].set_yticklabels(value_counts.index)\n",
    "            axes[0].set_xlabel('Count')\n",
    "            axes[0].set_title(f'{feature} - Category Distribution')\n",
    "            \n",
    "            category_means = train.groupby(feature)[target_col].mean().sort_values(ascending=True)\n",
    "            axes[1].barh(range(len(category_means)), category_means.values)\n",
    "            axes[1].set_yticks(range(len(category_means)))\n",
    "            axes[1].set_yticklabels(category_means.index)\n",
    "            axes[1].set_xlabel('Average Milk Production')\n",
    "            axes[1].set_title(f'{feature} - Average Milk Production by Category')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a00d67",
   "metadata": {},
   "source": [
    "## Outlier Detection\n",
    "\n",
    "Using the IQR method to detect outliers:\n",
    "- IQR = Q3 - Q1\n",
    "- Lower Bound = Q1 - 1.5 * IQR\n",
    "- Upper Bound = Q3 + 1.5 * IQR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9bdc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(numeric_features) > 0:\n",
    "    outlier_summary = []\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "        Q1 = train[feature].quantile(0.25)\n",
    "        Q3 = train[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = train[(train[feature] < lower_bound) | (train[feature] > upper_bound)]\n",
    "        n_outliers = len(outliers)\n",
    "        outlier_percent = (n_outliers / len(train)) * 100\n",
    "        \n",
    "        if n_outliers > 0:\n",
    "            outlier_summary.append({\n",
    "                'feature': feature,\n",
    "                'n_outliers': n_outliers,\n",
    "                'percent': outlier_percent,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound,\n",
    "                'min_value': train[feature].min(),\n",
    "                'max_value': train[feature].max()\n",
    "            })\n",
    "    \n",
    "    if outlier_summary:\n",
    "        outlier_df = pd.DataFrame(outlier_summary).sort_values('percent', ascending=False)\n",
    "        print(\"=\" * 80)\n",
    "        print(\"Outlier Detection Results (IQR Method)\")\n",
    "        print(\"=\" * 80)\n",
    "        print(outlier_df.to_string(index=False))\n",
    "        \n",
    "        top_outliers = outlier_df.head(6)\n",
    "        if len(top_outliers) > 0:\n",
    "            n_plots = len(top_outliers)\n",
    "            n_cols = 3\n",
    "            n_rows = (n_plots + n_cols - 1) // n_cols\n",
    "            \n",
    "            fig, axes = plt.subplots(n_rows, n_cols, figsize=(15, 4*n_rows))\n",
    "            if n_rows == 1:\n",
    "                axes = [axes] if n_plots == 1 else axes\n",
    "            else:\n",
    "                axes = axes.flatten()\n",
    "            \n",
    "            for plot_idx, (df_idx, row) in enumerate(top_outliers.iterrows()):\n",
    "                feature = row['feature']\n",
    "                axes[plot_idx].boxplot(train[feature].dropna(), vert=True)\n",
    "                axes[plot_idx].set_ylabel(feature)\n",
    "                axes[plot_idx].set_title(f'{feature}\\nOutliers: {row[\"n_outliers\"]} ({row[\"percent\"]:.2f}%)')\n",
    "                axes[plot_idx].grid(axis='y', alpha=0.3)\n",
    "            \n",
    "            for idx in range(n_plots, len(axes)):\n",
    "                axes[idx].axis('off')\n",
    "            \n",
    "            plt.suptitle('Features with Most Outliers (Boxplot)', y=1.00, fontsize=14)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    else:\n",
    "        print(\"No outliers detected using the IQR method\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7570812b",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning\n",
    "\n",
    "In this section, we will:\n",
    "1. Handle missing values (different strategies)\n",
    "2. Handle outliers (truncation or removal)\n",
    "3. Optimize data types to save memory\n",
    "\n",
    "**Strategies**:\n",
    "- **Missing values**: \n",
    "  - Numerical: Choose mean/median based on skewness\n",
    "  - Categorical: Use mode\n",
    "  - Create missing value indicators\n",
    "- **Outliers**: \n",
    "  - Use Winsorization (truncation) method\n",
    "  - Only handle highly skewed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f4b5e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_threshold = 5\n",
    "\n",
    "for col in train.columns:\n",
    "    missing_pct = (train[col].isnull().sum() / len(train)) * 100\n",
    "    if missing_pct > missing_threshold:\n",
    "        indicator_name = f'{col}_missing'\n",
    "        train[indicator_name] = train[col].isnull().astype(int)\n",
    "        test[indicator_name] = test[col].isnull().astype(int)\n",
    "        print(f\"Created missing value indicator: {indicator_name} (Missing rate: {missing_pct:.2f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b55382",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in numeric_features:\n",
    "    if train[feature].isnull().sum() > 0:\n",
    "        skewness = train[feature].skew()\n",
    "        \n",
    "        if abs(skewness) > 1:\n",
    "            fill_value = train[feature].median()\n",
    "            strategy = \"Median (skewed distribution)\"\n",
    "        else:\n",
    "            fill_value = train[feature].mean()\n",
    "            strategy = \"Mean (normal distribution)\"\n",
    "        \n",
    "        train[feature].fillna(fill_value, inplace=True)\n",
    "        test[feature].fillna(fill_value, inplace=True)\n",
    "        \n",
    "        print(f\"{feature}: Filled using {strategy}, Value={fill_value:.4f}, Skewness={skewness:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188844d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in categorical_features:\n",
    "    if train[feature].isnull().sum() > 0:\n",
    "        mode_value = train[feature].mode()[0]\n",
    "        train[feature].fillna(mode_value, inplace=True)\n",
    "        test[feature].fillna(mode_value, inplace=True)\n",
    "        print(f\"{feature}: Filled using mode, Value={mode_value}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Missing value imputation completed\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Remaining missing values in training set: {train.isnull().sum().sum()}\")\n",
    "print(f\"Remaining missing values in test set: {test.isnull().sum().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05295dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_treatment_summary = []\n",
    "\n",
    "for feature in numeric_features:\n",
    "    skewness = abs(train[feature].skew())\n",
    "    \n",
    "    if skewness > 2:\n",
    "        Q1 = train[feature].quantile(0.25)\n",
    "        Q3 = train[feature].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        n_outliers_before = ((train[feature] < lower_bound) | (train[feature] > upper_bound)).sum()\n",
    "        \n",
    "        if n_outliers_before > 0:\n",
    "            train[feature] = train[feature].clip(lower=lower_bound, upper=upper_bound)\n",
    "            test[feature] = test[feature].clip(lower=lower_bound, upper=upper_bound)\n",
    "            \n",
    "            outlier_treatment_summary.append({\n",
    "                'feature': feature,\n",
    "                'skewness': skewness,\n",
    "                'n_outliers': n_outliers_before,\n",
    "                'lower_bound': lower_bound,\n",
    "                'upper_bound': upper_bound\n",
    "            })\n",
    "\n",
    "if outlier_treatment_summary:\n",
    "    outlier_df = pd.DataFrame(outlier_treatment_summary)\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Outlier Treatment (Winsorization) - Only for Highly Skewed Features (Skewness > 2)\")\n",
    "    print(\"=\" * 80)\n",
    "    print(outlier_df.to_string(index=False))\n",
    "else:\n",
    "    print(\"No outlier treatment needed (no highly skewed features)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d63d7151",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in train.select_dtypes(include=['int64']).columns:\n",
    "    if col != id_col:\n",
    "        c_min = train[col].min()\n",
    "        c_max = train[col].max()\n",
    "        \n",
    "        if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n",
    "            train[col] = train[col].astype(np.int8)\n",
    "            test[col] = test[col].astype(np.int8)\n",
    "        elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n",
    "            train[col] = train[col].astype(np.int16)\n",
    "            test[col] = test[col].astype(np.int16)\n",
    "        elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n",
    "            train[col] = train[col].astype(np.int32)\n",
    "            test[col] = test[col].astype(np.int32)\n",
    "\n",
    "for col in train.select_dtypes(include=['float64']).columns:\n",
    "    if col != target_col:\n",
    "        c_min = train[col].min()\n",
    "        c_max = train[col].max()\n",
    "        \n",
    "        if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n",
    "            train[col] = train[col].astype(np.float32)\n",
    "            test[col] = test[col].astype(np.float32)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Data type optimization completed\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549b8a20",
   "metadata": {},
   "source": [
    "## 3. Feature Engineering\n",
    "\n",
    "Feature engineering is key to improving model performance! We will implement:\n",
    "\n",
    "1. **Frequency Encoding** - Replace categorical values with their occurrence frequency\n",
    "2. **Target Encoding** - Encode using the mean of the target variable\n",
    "3. **Label Encoding** - Convert categories to numbers (for tree models)\n",
    "4. **Interaction Features** - Create interaction terms between features\n",
    "5. **Polynomial Features** - Create squares, cubes, etc. of features\n",
    "6. **Statistical Features** - Grouped statistics by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f757a5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\" * 60)\n",
    "print(\"1. Frequency Encoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for feature in categorical_features:\n",
    "    freq_encoding = train[feature].value_counts(normalize=True).to_dict()\n",
    "    \n",
    "    new_feature_name = f'{feature}_freq'\n",
    "    train[new_feature_name] = train[feature].map(freq_encoding)\n",
    "    test[new_feature_name] = test[feature].map(freq_encoding)\n",
    "    \n",
    "    min_freq = train[new_feature_name].min()\n",
    "    test[new_feature_name].fillna(min_freq, inplace=True)\n",
    "    \n",
    "    print(f\"{feature} -> {new_feature_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b025986",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"2. Target Encoding with Smoothing\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "smoothing = 10\n",
    "global_mean = train[target_col].mean()\n",
    "\n",
    "for feature in categorical_features:\n",
    "    agg = train.groupby(feature)[target_col].agg(['mean', 'count'])\n",
    "    \n",
    "    smoothed_mean = (agg['mean'] * agg['count'] + global_mean * smoothing) / (agg['count'] + smoothing)\n",
    "    target_encoding = smoothed_mean.to_dict()\n",
    "    \n",
    "    new_feature_name = f'{feature}_target'\n",
    "    train[new_feature_name] = train[feature].map(target_encoding)\n",
    "    test[new_feature_name] = test[feature].map(target_encoding)\n",
    "    \n",
    "    test[new_feature_name].fillna(global_mean, inplace=True)\n",
    "    \n",
    "    print(f\"{feature} -> {new_feature_name} (smoothing={smoothing})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0acd96",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"3. Label Encoding\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "label_encoders = {}\n",
    "\n",
    "for feature in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    \n",
    "    combined = pd.concat([train[feature], test[feature]], axis=0)\n",
    "    le.fit(combined)\n",
    "    \n",
    "    new_feature_name = f'{feature}_label'\n",
    "    train[new_feature_name] = le.transform(train[feature])\n",
    "    test[new_feature_name] = le.transform(test[feature])\n",
    "    \n",
    "    label_encoders[feature] = le\n",
    "    print(f\"{feature} -> {new_feature_name} ({len(le.classes_)} categories)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31cfb764",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"4. Feature Interactions\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "numeric_features_updated = train.select_dtypes(include=['int8', 'int16', 'int32', 'int64', \n",
    "                                                         'float32', 'float64']).columns.tolist()\n",
    "if target_col in numeric_features_updated:\n",
    "    numeric_features_updated.remove(target_col)\n",
    "if id_col in numeric_features_updated:\n",
    "    numeric_features_updated.remove(id_col)\n",
    "\n",
    "if len(numeric_features_updated) > 0:\n",
    "    correlations = train[numeric_features_updated + [target_col]].corr()[target_col].drop(target_col)\n",
    "    top_features = correlations.abs().sort_values(ascending=False).head(5).index.tolist()\n",
    "    \n",
    "    print(f\"Selected top 5 features: {top_features}\")\n",
    "    \n",
    "    interaction_count = 0\n",
    "    for i in range(len(top_features)):\n",
    "        for j in range(i+1, len(top_features)):\n",
    "            feat1, feat2 = top_features[i], top_features[j]\n",
    "            \n",
    "            train[f'{feat1}_x_{feat2}'] = train[feat1] * train[feat2]\n",
    "            test[f'{feat1}_x_{feat2}'] = test[feat1] * test[feat2]\n",
    "            \n",
    "            train[f'{feat1}_div_{feat2}'] = train[feat1] / (train[feat2] + 1e-5)\n",
    "            test[f'{feat1}_div_{feat2}'] = test[feat1] / (test[feat2] + 1e-5)\n",
    "            \n",
    "            train[f'{feat1}_plus_{feat2}'] = train[feat1] + train[feat2]\n",
    "            test[f'{feat1}_plus_{feat2}'] = test[feat1] + test[feat2]\n",
    "            \n",
    "            train[f'{feat1}_minus_{feat2}'] = train[feat1] - train[feat2]\n",
    "            test[f'{feat1}_minus_{feat2}'] = test[feat1] - test[feat2]\n",
    "            \n",
    "            interaction_count += 4\n",
    "    \n",
    "    print(f\"Created {interaction_count} interaction features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5dd0639",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"5. Polynomial Features\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(numeric_features_updated) > 0:\n",
    "    correlations = train[numeric_features_updated + [target_col]].corr()[target_col].drop(target_col)\n",
    "    top_10_features = correlations.abs().sort_values(ascending=False).head(10).index.tolist()\n",
    "    \n",
    "    print(f\"Selected top 10 features: {top_10_features}\")\n",
    "    \n",
    "    for feature in top_10_features:\n",
    "        train[f'{feature}_squared'] = train[feature] ** 2\n",
    "        test[f'{feature}_squared'] = test[feature] ** 2\n",
    "        \n",
    "        train[f'{feature}_cubed'] = train[feature] ** 3\n",
    "        test[f'{feature}_cubed'] = test[feature] ** 3\n",
    "        \n",
    "        if train[feature].min() >= 0:\n",
    "            train[f'{feature}_sqrt'] = np.sqrt(train[feature])\n",
    "            test[f'{feature}_sqrt'] = np.sqrt(test[feature])\n",
    "    \n",
    "    print(f\"Created polynomial features (squared, cubed, square root) for top 10 features\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792e01db",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"6. Statistical Aggregations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if len(categorical_features) > 0 and len(numeric_features_updated) > 0:\n",
    "    top_numeric = correlations.abs().sort_values(ascending=False).head(5).index.tolist()\n",
    "    \n",
    "    for cat_feature in categorical_features[:min(3, len(categorical_features))]:\n",
    "        for num_feature in top_numeric:\n",
    "            grouped_mean = train.groupby(cat_feature)[num_feature].mean()\n",
    "            train[f'{cat_feature}_{num_feature}_mean'] = train[cat_feature].map(grouped_mean)\n",
    "            test[f'{cat_feature}_{num_feature}_mean'] = test[cat_feature].map(grouped_mean)\n",
    "            test[f'{cat_feature}_{num_feature}_mean'].fillna(train[num_feature].mean(), inplace=True)\n",
    "            \n",
    "            grouped_std = train.groupby(cat_feature)[num_feature].std()\n",
    "            train[f'{cat_feature}_{num_feature}_std'] = train[cat_feature].map(grouped_std)\n",
    "            test[f'{cat_feature}_{num_feature}_std'] = test[cat_feature].map(grouped_std)\n",
    "            test[f'{cat_feature}_{num_feature}_std'].fillna(train[num_feature].std(), inplace=True)\n",
    "            \n",
    "            grouped_median = train.groupby(cat_feature)[num_feature].median()\n",
    "            train[f'{cat_feature}_{num_feature}_median'] = train[cat_feature].map(grouped_median)\n",
    "            test[f'{cat_feature}_{num_feature}_median'] = test[cat_feature].map(grouped_median)\n",
    "            test[f'{cat_feature}_{num_feature}_median'].fillna(train[num_feature].median(), inplace=True)\n",
    "    \n",
    "    print(f\"Created statistical aggregation features (mean, std, median)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad29ae18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(PROCESSED_DATA_DIR / 'train_processed.csv', index=False)\n",
    "test.to_csv(PROCESSED_DATA_DIR / 'test_processed.csv', index=False)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Data saved successfully\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set shape: {train.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")\n",
    "print(f\"Number of features increased: {train.shape[1] - len(numeric_features) - len(categorical_features) - 2}\")\n",
    "print(f\"\\nSave paths:\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'train_processed.csv'}\")\n",
    "print(f\"  - {PROCESSED_DATA_DIR / 'test_processed.csv'}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982586a4",
   "metadata": {},
   "source": [
    "## 4. Feature Selection & Dimensionality Reduction\n",
    "\n",
    "Before training the model, we need to:\n",
    "1. Remove original categorical features (already encoded)\n",
    "2. Prepare training and validation sets\n",
    "3. Remove highly correlated features (to avoid multicollinearity)\n",
    "4. Feature scaling (standardization)\n",
    "5. PCA dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64d6504",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = train.select_dtypes(include=['object']).columns.tolist()\n",
    "if id_col in categorical_features:\n",
    "    categorical_features.remove(id_col)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Remove original categorical features (already encoded)\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Features to remove: {categorical_features}\")\n",
    "\n",
    "train.drop(columns=categorical_features, inplace=True, errors='ignore')\n",
    "test.drop(columns=categorical_features, inplace=True, errors='ignore')\n",
    "\n",
    "print(f\"Removal completed\")\n",
    "print(f\"Training set shape: {train.shape}\")\n",
    "print(f\"Test set shape: {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feedfe9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(columns=[target_col, id_col], errors='ignore')\n",
    "y = train[target_col]\n",
    "X_test = test.drop(columns=[id_col], errors='ignore')\n",
    "test_ids = test[id_col]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Features and target variable preparation completed\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Feature matrix X shape: {X.shape}\")\n",
    "print(f\"Target variable y shape: {y.shape}\")\n",
    "print(f\"Test set X_test shape: {X_test.shape}\")\n",
    "print(f\"Number of features: {X.shape[1]}\")\n",
    "\n",
    "missing_cols = set(X.columns) - set(X_test.columns)\n",
    "extra_cols = set(X_test.columns) - set(X.columns)\n",
    "\n",
    "if missing_cols:\n",
    "    print(f\"Test set is missing columns: {missing_cols}\")\n",
    "    for col in missing_cols:\n",
    "        X_test[col] = 0\n",
    "\n",
    "if extra_cols:\n",
    "    print(f\"Test set has extra columns: {extra_cols}\")\n",
    "    X_test = X_test[X.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477b09ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Dataset split completed\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Test set: {X_test.shape}\")\n",
    "print(f\"\\nTraining set proportion: {len(X_train)/len(X)*100:.1f}%\")\n",
    "print(f\"Validation set proportion: {len(X_val)/len(X)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b52a7c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix = X_train.corr().abs()\n",
    "\n",
    "upper_triangle = correlation_matrix.where(\n",
    "    np.triu(np.ones(correlation_matrix.shape), k=1).astype(bool)\n",
    ")\n",
    "\n",
    "high_corr_threshold = 0.95\n",
    "to_drop = [column for column in upper_triangle.columns \n",
    "           if any(upper_triangle[column] > high_corr_threshold)]\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"Removing highly correlated features (correlation > {high_corr_threshold})\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Found {len(to_drop)} highly correlated features\")\n",
    "\n",
    "if to_drop:\n",
    "    print(f\"Features to drop: {to_drop[:10]}...\")\n",
    "    \n",
    "    X_train = X_train.drop(columns=to_drop)\n",
    "    X_val = X_val.drop(columns=to_drop)\n",
    "    X_test = X_test.drop(columns=to_drop)\n",
    "    \n",
    "    print(f\"Removal completed\")\n",
    "    print(f\"New training set shape: {X_train.shape}\")\n",
    "else:\n",
    "    print(\"No highly correlated features found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74edfeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"StandardScaler\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled = scaler.transform(X_val)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_train_scaled = pd.DataFrame(X_train_scaled, columns=X_train.columns, index=X_train.index)\n",
    "X_val_scaled = pd.DataFrame(X_val_scaled, columns=X_val.columns, index=X_val.index)\n",
    "X_test_scaled = pd.DataFrame(X_test_scaled, columns=X_test.columns)\n",
    "\n",
    "print(\"StandardScaler completed\")\n",
    "print(f\"Training set mean: {X_train_scaled.mean().mean():.6f}\")\n",
    "print(f\"Training set std deviation: {X_train_scaled.std().mean():.6f}\")\n",
    "\n",
    "joblib.dump(scaler, MODEL_DIR / 'scaler.pkl')\n",
    "print(f\"Scaler saved to: {MODEL_DIR / 'scaler.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c62a12",
   "metadata": {},
   "source": [
    "## Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) can:\n",
    "1. Reduce feature dimensionality\n",
    "2. Remove noise\n",
    "3. Visualize feature importance\n",
    "4. Accelerate model training\n",
    "\n",
    "We will analyze how many principal components are needed to retain 95% of the variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44fbe5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_full = PCA(random_state=RANDOM_STATE)\n",
    "pca_full.fit(X_train_scaled)\n",
    "\n",
    "cumsum_variance = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "\n",
    "n_components_95 = np.argmax(cumsum_variance >= 0.95) + 1\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PCA Analysis Results\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Number of principal components to retain 95% variance: {n_components_95}\")\n",
    "print(f\"Dimensionality reduction ratio: {(1 - n_components_95/X_train_scaled.shape[1])*100:.1f}%\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "axes[0].plot(range(1, min(51, len(pca_full.explained_variance_ratio_)+1)), \n",
    "             pca_full.explained_variance_ratio_[:50], 'bo-')\n",
    "axes[0].set_xlabel('Principal Component')\n",
    "axes[0].set_ylabel('Explained Variance Ratio')\n",
    "axes[0].set_title('Scree Plot - First 50 Principal Components')\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "axes[1].plot(range(1, min(101, len(cumsum_variance)+1)), \n",
    "             cumsum_variance[:100], 'ro-')\n",
    "axes[1].axhline(y=0.95, color='green', linestyle='--', label='95% Variance')\n",
    "axes[1].axvline(x=n_components_95, color='blue', linestyle='--', \n",
    "                label=f'{n_components_95} Principal Components')\n",
    "axes[1].set_xlabel('Number of Principal Components')\n",
    "axes[1].set_ylabel('Cumulative Explained Variance Ratio')\n",
    "axes[1].set_title('Cumulative Explained Variance Plot')\n",
    "axes[1].legend()\n",
    "axes[1].grid(alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2969366",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=n_components_95, random_state=RANDOM_STATE)\n",
    "\n",
    "X_train_pca = pca.fit_transform(X_train_scaled)\n",
    "X_val_pca = pca.transform(X_val_scaled)\n",
    "X_test_pca = pca.transform(X_test_scaled)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"PCA Dimensionality Reduction Completed\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Original number of features: {X_train_scaled.shape[1]}\")\n",
    "print(f\"Number of features after reduction: {X_train_pca.shape[1]}\")\n",
    "print(f\"Variance retained: {pca.explained_variance_ratio_.sum()*100:.2f}%\")\n",
    "joblib.dump(pca, MODEL_DIR / 'pca.pkl')\n",
    "print(f\"PCA model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "922710d0",
   "metadata": {},
   "source": [
    "## 5. Model Training\n",
    "\n",
    "We will train the following 7 regression models:\n",
    "\n",
    "1. **Linear Regression** - The simplest baseline model\n",
    "2. **Ridge Regression** - L2 regularization to prevent overfitting\n",
    "3. **Lasso Regression** - L1 regularization for feature selection\n",
    "4. **Decision Tree** - Non-linear model\n",
    "5. **K-Nearest Neighbors (KNN)** - Distance-based method\n",
    "6. **Support Vector Regression (SVR)** - Support vector regression\n",
    "7. **Neural Network (MLP)** - Multi-layer perceptron\n",
    "\n",
    "Each model will undergo hyperparameter tuning and cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f11de2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, X_train, y_train, X_val, y_val, model_name):\n",
    "    y_train_pred = model.predict(X_train)\n",
    "    train_rmse = np.sqrt(mean_squared_error(y_train, y_train_pred))\n",
    "    train_mae = mean_absolute_error(y_train, y_train_pred)\n",
    "    train_r2 = r2_score(y_train, y_train_pred)\n",
    "    \n",
    "    y_val_pred = model.predict(X_val)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred))\n",
    "    val_mae = mean_absolute_error(y_val, y_val_pred)\n",
    "    val_r2 = r2_score(y_val, y_val_pred)\n",
    "    \n",
    "    overfitting = train_rmse < val_rmse * 0.9\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(f\"{model_name} - Evaluation Results\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"Training RMSE: {train_rmse:.4f}\")\n",
    "    print(f\"Validation RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"Training MAE:  {train_mae:.4f}\")\n",
    "    print(f\"Validation MAE:  {val_mae:.4f}\")\n",
    "    print(f\"Training R²:   {train_r2:.4f}\")\n",
    "    print(f\"Validation R²:   {val_r2:.4f}\")\n",
    "    \n",
    "    if overfitting:\n",
    "        print(f\"Warning: Possible overfitting detected!\")\n",
    "    else:\n",
    "        print(f\"Model generalizes well\")\n",
    "    \n",
    "    return {\n",
    "        'model_name': model_name,\n",
    "        'train_rmse': train_rmse,\n",
    "        'val_rmse': val_rmse,\n",
    "        'train_mae': train_mae,\n",
    "        'val_mae': val_mae,\n",
    "        'train_r2': train_r2,\n",
    "        'val_r2': val_r2,\n",
    "        'overfitting': overfitting\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f1764c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_models_results = []\n",
    "trained_models = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aac31d",
   "metadata": {},
   "source": [
    "### 1. Linear Regression\n",
    "\n",
    "The most basic regression model, using the least squares method.\n",
    "- **Advantages**: Simple, highly interpretable\n",
    "- **Disadvantages**: Assumes a linear relationship, may underfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6af135",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Training Linear Regression model...\\n\")\n",
    "\n",
    "lr_model = LinearRegression()\n",
    "lr_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lr_results = evaluate_model(lr_model, X_train_scaled, y_train, \n",
    "                             X_val_scaled, y_val, \"Linear Regression\")\n",
    "\n",
    "all_models_results.append(lr_results)\n",
    "trained_models['linear_regression'] = lr_model\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'coefficient': np.abs(lr_model.coef_)\n",
    "}).sort_values('coefficient', ascending=False).head(10)\n",
    "\n",
    "print(f\"\\nTop 10 most important features (by absolute coefficient value):\")\n",
    "print(feature_importance.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a019d36",
   "metadata": {},
   "source": [
    "### 2. Ridge Regression (L2 Regularization)\n",
    "\n",
    "Ridge regression adds an L2 regularization term to prevent overfitting.\n",
    "- **Regularization term**: λ * Σ(w²)\n",
    "- **Hyperparameter to tune**: alpha (λ)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57120e61",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Training Ridge Regression model (alpha selected by cross-validation)...\\n\")\n",
    "\n",
    "alphas = [0.001, 0.01, 0.1, 1, 10, 100, 1000]\n",
    "cv_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    ridge = Ridge(alpha=alpha, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(ridge, X_train_scaled, y_train, \n",
    "                            cv=5, scoring='neg_root_mean_squared_error')\n",
    "    cv_scores.append(-scores.mean())\n",
    "\n",
    "best_alpha = alphas[np.argmin(cv_scores)]\n",
    "\n",
    "print(f\"Alpha test results:\")\n",
    "for alpha, score in zip(alphas, cv_scores):\n",
    "    marker = \" ← Best\" if alpha == best_alpha else \"\"\n",
    "    print(f\"  alpha={alpha:7.3f}, CV RMSE={score:.4f}{marker}\")\n",
    "\n",
    "ridge_model = Ridge(alpha=best_alpha, random_state=RANDOM_STATE)\n",
    "ridge_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "ridge_results = evaluate_model(ridge_model, X_train_scaled, y_train, \n",
    "                               X_val_scaled, y_val, \"Ridge Regression\")\n",
    "\n",
    "all_models_results.append(ridge_results)\n",
    "trained_models['ridge'] = ridge_model\n",
    "\n",
    "print(f\"\\nBest alpha: {best_alpha}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7e23fd6",
   "metadata": {},
   "source": [
    "### 3. Lasso Regression (L1 Regularization)\n",
    "\n",
    "Lasso regression uses L1 regularization and can perform feature selection.\n",
    "- **Regularization term**: λ * Σ|w|\n",
    "- **Characteristic**: Can shrink some coefficients to zero, achieving feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc02cdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Training Lasso Regression model (alpha selected by cross-validation)...\\n\")\n",
    "\n",
    "alphas_lasso = [0.001, 0.01, 0.1, 1, 10]\n",
    "cv_scores_lasso = []\n",
    "\n",
    "for alpha in alphas_lasso:\n",
    "    lasso = Lasso(alpha=alpha, random_state=RANDOM_STATE, max_iter=10000)\n",
    "    scores = cross_val_score(lasso, X_train_scaled, y_train, \n",
    "                            cv=5, scoring='neg_root_mean_squared_error')\n",
    "    cv_scores_lasso.append(-scores.mean())\n",
    "\n",
    "best_alpha_lasso = alphas_lasso[np.argmin(cv_scores_lasso)]\n",
    "\n",
    "print(f\"Alpha test results:\")\n",
    "for alpha, score in zip(alphas_lasso, cv_scores_lasso):\n",
    "    marker = \" ← Best\" if alpha == best_alpha_lasso else \"\"\n",
    "    print(f\"  alpha={alpha:7.3f}, CV RMSE={score:.4f}{marker}\")\n",
    "\n",
    "lasso_model = Lasso(alpha=best_alpha_lasso, random_state=RANDOM_STATE, max_iter=10000)\n",
    "lasso_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "lasso_results = evaluate_model(lasso_model, X_train_scaled, y_train, \n",
    "                               X_val_scaled, y_val, \"Lasso Regression\")\n",
    "\n",
    "all_models_results.append(lasso_results)\n",
    "trained_models['lasso'] = lasso_model\n",
    "\n",
    "n_nonzero = np.sum(lasso_model.coef_ != 0)\n",
    "print(f\"\\nFeature selection results:\")\n",
    "print(f\"  Original number of features: {len(lasso_model.coef_)}\")\n",
    "print(f\"  Number of non-zero coefficients: {n_nonzero}\")\n",
    "print(f\"  Number of features removed: {len(lasso_model.coef_) - n_nonzero}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0772eed",
   "metadata": {},
   "source": [
    "### 4. Decision Tree Regression\n",
    "\n",
    "Decision trees make predictions by recursively partitioning the feature space.\n",
    "- **Splitting criterion**: MSE (Mean Squared Error)\n",
    "- **Important parameter**: max_depth (maximum depth of the tree)\n",
    "- **Advantages**: Can capture nonlinear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b7dc97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Training Decision Tree model (selecting depth by cross-validation)...\\n\")\n",
    "\n",
    "max_depths = [3, 5, 7, 10, 15, 20, None]\n",
    "cv_scores_dt = []\n",
    "\n",
    "for depth in max_depths:\n",
    "    dt = DecisionTreeRegressor(max_depth=depth, random_state=RANDOM_STATE)\n",
    "    scores = cross_val_score(dt, X_train_scaled, y_train, \n",
    "                            cv=5, scoring='neg_root_mean_squared_error')\n",
    "    cv_scores_dt.append(-scores.mean())\n",
    "\n",
    "best_depth = max_depths[np.argmin(cv_scores_dt)]\n",
    "\n",
    "print(f\"Max depth test results:\")\n",
    "for depth, score in zip(max_depths, cv_scores_dt):\n",
    "    marker = \" ← Best\" if depth == best_depth else \"\"\n",
    "    depth_str = \"None (unlimited)\" if depth is None else str(depth)\n",
    "    print(f\"  max_depth={depth_str:15s}, CV RMSE={score:.4f}{marker}\")\n",
    "\n",
    "dt_model = DecisionTreeRegressor(max_depth=best_depth, random_state=RANDOM_STATE)\n",
    "dt_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "dt_results = evaluate_model(dt_model, X_train_scaled, y_train, \n",
    "                            X_val_scaled, y_val, \"Decision Tree\")\n",
    "\n",
    "all_models_results.append(dt_results)\n",
    "trained_models['decision_tree'] = dt_model\n",
    "\n",
    "feature_importance_dt = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': dt_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "\n",
    "print(f\"\\nTop 10 Important Features (Decision Tree):\")\n",
    "print(feature_importance_dt.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "558de100",
   "metadata": {},
   "source": [
    "### 5. K-Nearest Neighbors (KNN) Regression\n",
    "\n",
    "KNN predicts the target value by finding the k nearest neighbors.\n",
    "- **Distance Metric**: Euclidean distance\n",
    "- **Key Parameter**: k (number of neighbors)\n",
    "- **Prediction**: Average value of the k nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e00c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Training KNN model (selecting k by cross-validation)...\\n\")\n",
    "\n",
    "k_values = [3, 5, 7, 10, 15, 20, 25, 30]\n",
    "cv_scores_knn = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsRegressor(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X_train_scaled, y_train, \n",
    "                            cv=5, scoring='neg_root_mean_squared_error')\n",
    "    cv_scores_knn.append(-scores.mean())\n",
    "\n",
    "best_k = k_values[np.argmin(cv_scores_knn)]\n",
    "\n",
    "print(f\"k value test results:\")\n",
    "for k, score in zip(k_values, cv_scores_knn):\n",
    "    marker = \" ← Best\" if k == best_k else \"\"\n",
    "    print(f\"  k={k:3d}, CV RMSE={score:.4f}{marker}\")\n",
    "\n",
    "knn_model = KNeighborsRegressor(n_neighbors=best_k)\n",
    "knn_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "knn_results = evaluate_model(knn_model, X_train_scaled, y_train, \n",
    "                             X_val_scaled, y_val, \"KNN\")\n",
    "\n",
    "all_models_results.append(knn_results)\n",
    "trained_models['knn'] = knn_model\n",
    "\n",
    "print(f\"\\nBest k value: {best_k}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bc1e52",
   "metadata": {},
   "source": [
    "### 6. Support Vector Regression (SVR)\n",
    "\n",
    "SVR uses kernel tricks to map data into a high-dimensional space.\n",
    "- **Kernel Function**: RBF (Radial Basis Function)\n",
    "- **Characteristics**: Suitable for small to medium-sized datasets\n",
    "- **Note**: May be slow for large datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9d1608",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Training SVR model (RBF kernel)...\\n\")\n",
    "\n",
    "sample_size = min(10000, len(X_train_scaled))\n",
    "sample_indices = np.random.choice(len(X_train_scaled), sample_size, replace=False)\n",
    "\n",
    "X_train_sample = X_train_scaled.iloc[sample_indices]\n",
    "y_train_sample = y_train.iloc[sample_indices]\n",
    "\n",
    "print(f\"Using {sample_size} samples to train SVR (accelerated training)\")\n",
    "\n",
    "svr_model = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr_model.fit(X_train_sample, y_train_sample)\n",
    "\n",
    "svr_results = evaluate_model(svr_model, X_train_scaled, y_train, \n",
    "                             X_val_scaled, y_val, \"SVR\")\n",
    "\n",
    "all_models_results.append(svr_results)\n",
    "trained_models['svr'] = svr_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab7a134",
   "metadata": {},
   "source": [
    "### 7. Neural Network Regression (MLP)\n",
    "\n",
    "Neural networks can learn complex nonlinear patterns.\n",
    "- **Architecture**: Input layer → Hidden layers → Output layer\n",
    "- **Activation Function**: ReLU\n",
    "- **Optimizer**: Adam\n",
    "- **Regularization**: L2 regularization + Early Stopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "900ec29a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"MLP\\n\")\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Use device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU model: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\\n\")\n",
    "\n",
    "class MLPRegressorPyTorch(nn.Module):\n",
    "    def __init__(self, input_size, hidden_layers):\n",
    "        super(MLPRegressorPyTorch, self).__init__()\n",
    "        layers = []\n",
    "        prev_size = input_size\n",
    "        \n",
    "        for hidden_size in hidden_layers:\n",
    "            layers.append(nn.Linear(prev_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            layers.append(nn.Dropout(0.2))\n",
    "            prev_size = hidden_size\n",
    "        \n",
    "        layers.append(nn.Linear(prev_size, 1))\n",
    "        self.network = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "def train_pytorch_mlp(model, train_loader, val_loader, epochs=200, lr=0.001, patience=10, use_validation=True):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=0.001)\n",
    "    \n",
    "    best_train_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "    epochs_trained = 0\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += loss.item() * X_batch.size(0)\n",
    "        \n",
    "        train_loss /= len(train_loader.dataset)\n",
    "        epochs_trained = epoch + 1\n",
    "        \n",
    "        if use_validation and val_loader is not train_loader:\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for X_batch, y_batch in val_loader:\n",
    "                    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                    outputs = model(X_batch)\n",
    "                    loss = criterion(outputs, y_batch)\n",
    "                    val_loss += loss.item() * X_batch.size(0)\n",
    "            val_loss /= len(val_loader.dataset)\n",
    "            monitor_loss = val_loss\n",
    "        else:\n",
    "            monitor_loss = train_loss\n",
    "        \n",
    "        if monitor_loss < best_train_loss:\n",
    "            best_train_loss = monitor_loss\n",
    "            patience_counter = 0\n",
    "            best_model_state = model.state_dict().copy()\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "        \n",
    "        if patience_counter >= patience:\n",
    "            model.load_state_dict(best_model_state)\n",
    "            break\n",
    "    \n",
    "    return model, best_train_loss, epochs_trained\n",
    "\n",
    "architectures = [(100,), (100, 50), (100, 100), (100, 50, 25)]\n",
    "input_size = X_train_scaled.shape[1]\n",
    "\n",
    "X_train_tensor = torch.FloatTensor(X_train_scaled.values)\n",
    "y_train_tensor = torch.FloatTensor(y_train.values).reshape(-1, 1)\n",
    "X_val_tensor = torch.FloatTensor(X_val_scaled.values)\n",
    "y_val_tensor = torch.FloatTensor(y_val.values).reshape(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(X_val_tensor, y_val_tensor)\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256, shuffle=False)\n",
    "\n",
    "best_score = float('inf')\n",
    "best_architecture = None\n",
    "best_model_state = None\n",
    "\n",
    "print(\"Testing different network architectures:\")\n",
    "for arch in architectures:\n",
    "    model = MLPRegressorPyTorch(input_size, arch).to(device)\n",
    "    model, val_loss, n_epochs = train_pytorch_mlp(model, train_loader, val_loader, epochs=200, lr=0.001, patience=10)\n",
    "    \n",
    "    score = np.sqrt(val_loss)\n",
    "    \n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_architecture = arch\n",
    "        best_model_state = model.state_dict().copy()\n",
    "    \n",
    "    marker = \" ← Best\" if score == best_score else \"\"\n",
    "    print(f\"  Architecture={str(arch):20s}, Validation RMSE={score:.4f}, Epochs={n_epochs}{marker}\")\n",
    "\n",
    "print(f\"\\nUsing the best architecture: {best_architecture}\")\n",
    "\n",
    "mlp_model = MLPRegressorPyTorch(input_size, best_architecture).to(device)\n",
    "mlp_model.load_state_dict(best_model_state)\n",
    "final_loss = best_score ** 2\n",
    "final_epochs = 0\n",
    "\n",
    "class PyTorchModelWrapper:\n",
    "    def __init__(self, model, device):\n",
    "        self.model = model\n",
    "        self.device = device\n",
    "        self.loss_ = final_loss\n",
    "        self.n_iter_ = 500\n",
    "    \n",
    "    def predict(self, X):\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            X_tensor = torch.FloatTensor(X.values if hasattr(X, 'values') else X).to(self.device)\n",
    "            predictions = self.model(X_tensor).cpu().numpy().flatten()\n",
    "        return predictions\n",
    "\n",
    "mlp_model_wrapper = PyTorchModelWrapper(mlp_model, device)\n",
    "\n",
    "mlp_results = evaluate_model(mlp_model_wrapper, X_train_scaled, y_train, \n",
    "                             X_val_scaled, y_val, \"MLP\")\n",
    "\n",
    "all_models_results.append(mlp_results)\n",
    "trained_models['mlp'] = mlp_model_wrapper\n",
    "\n",
    "print(f\"\\nTraining info:\")\n",
    "print(f\"  Final loss: {mlp_model_wrapper.loss_:.4f}\")\n",
    "print(f\"  Best architecture: {best_architecture}\")\n",
    "print(f\"  Number of hidden layers: {len(best_architecture)}\")\n",
    "print(f\"  Device used: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d3db1c",
   "metadata": {},
   "source": [
    "## 6. Ensemble Learning\n",
    "\n",
    "Ensemble learning improves performance by combining multiple models. We will implement:\n",
    "\n",
    "1. **Random Forest** - Bagging method\n",
    "2. **Gradient Boosting** - Boosting method\n",
    "3. **AdaBoost** - Adaptive boosting\n",
    "4. **Voting Regressor** - Voting ensemble\n",
    "5. **Stacking Regressor** - Stacking ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d49b56b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"Random Forest\\n\")\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "rf_param_dist = {\n",
    "    'n_estimators': [150, 200, 250],\n",
    "    'max_depth': [12, 15, 18], \n",
    "    'min_samples_split': [20, 50, 100],\n",
    "    'min_samples_leaf': [10, 20, 30],\n",
    "    'max_features': [0.5, 0.7, 'sqrt'],\n",
    "    'max_samples': [0.7, 0.8, 0.9]\n",
    "}\n",
    "\n",
    "import os\n",
    "n_cores = max(1, os.cpu_count() - 2) if os.cpu_count() else 4\n",
    "\n",
    "rf = RandomForestRegressor(random_state=RANDOM_STATE, n_jobs=n_cores)\n",
    "\n",
    "rf_grid_search = RandomizedSearchCV(\n",
    "    rf, rf_param_dist, \n",
    "    n_iter=10,\n",
    "    cv=3, \n",
    "    scoring='neg_root_mean_squared_error',\n",
    "    n_jobs=1,\n",
    "    verbose=2,\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "rf_grid_search.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"\\nBest Parameters:\")\n",
    "for param, value in rf_grid_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")\n",
    "\n",
    "rf_model = rf_grid_search.best_estimator_\n",
    "\n",
    "rf_results = evaluate_model(rf_model, X_train_scaled, y_train, \n",
    "                            X_val_scaled, y_val, \"Random Forest\")\n",
    "\n",
    "all_models_results.append(rf_results)\n",
    "trained_models['random_forest'] = rf_model\n",
    "\n",
    "feature_importance_rf = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False).head(15)\n",
    "\n",
    "print(f\"\\nTop 15 Important Features (Random Forest):\")\n",
    "print(feature_importance_rf.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ce9b9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Gradient Boosting Regressor (faster version)...\\n\")\n",
    "gb_model = GradientBoostingRegressor(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.05,\n",
    "    max_depth=5,\n",
    "    min_samples_split=30,\n",
    "    min_samples_leaf=15,\n",
    "    subsample=0.8,\n",
    "    max_features='sqrt',\n",
    "    validation_fraction=0.1,\n",
    "    n_iter_no_change=20,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "gb_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "gb_results = evaluate_model(gb_model, X_train_scaled, y_train, \n",
    "                            X_val_scaled, y_val, \"Gradient Boosting\")\n",
    "\n",
    "all_models_results.append(gb_results)\n",
    "trained_models['gradient_boosting'] = gb_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12409a2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training AdaBoost Regressor (optimized version)...\\n\")\n",
    "\n",
    "ada_model = AdaBoostRegressor(\n",
    "    estimator=DecisionTreeRegressor(\n",
    "        max_depth=4,\n",
    "        min_samples_split=30,\n",
    "        min_samples_leaf=15,\n",
    "        max_features='sqrt',\n",
    "        random_state=RANDOM_STATE\n",
    "    ),\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.3,\n",
    "    loss='square',\n",
    "    random_state=RANDOM_STATE\n",
    ")\n",
    "\n",
    "ada_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "ada_results = evaluate_model(ada_model, X_train_scaled, y_train, \n",
    "                             X_val_scaled, y_val, \"AdaBoost\")\n",
    "\n",
    "all_models_results.append(ada_results)\n",
    "trained_models['adaboost'] = ada_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecd4791",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Voting Regressor (ensemble of Ridge, GB, RF, MLP)...\\n\")\n",
    "voting_model = VotingRegressor(\n",
    "    estimators=[\n",
    "        ('ridge', ridge_model),\n",
    "        ('gb', gb_model),\n",
    "        ('rf', rf_model),\n",
    "        ('mlp', mlp_model_wrapper)\n",
    "    ],\n",
    "    weights=[1, 2, 2, 1]\n",
    ")\n",
    "\n",
    "voting_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "voting_results = evaluate_model(voting_model, X_train_scaled, y_train, \n",
    "                                X_val_scaled, y_val, \"Voting Regressor\")\n",
    "\n",
    "all_models_results.append(voting_results)\n",
    "\n",
    "trained_models['voting'] = voting_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8a4139",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Stacking Regressor (faster version)...\\n\")\n",
    "base_learners = [\n",
    "    ('rf', RandomForestRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=8,\n",
    "        max_features='sqrt',\n",
    "        min_samples_split=20,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_STATE\n",
    "    )),\n",
    "    ('gb', GradientBoostingRegressor(\n",
    "        n_estimators=50,\n",
    "        max_depth=3,\n",
    "        learning_rate=0.2,\n",
    "        subsample=0.8,\n",
    "        max_features='sqrt',\n",
    "        random_state=RANDOM_STATE\n",
    "    )),\n",
    "    ('ridge', Ridge(alpha=best_alpha if 'best_alpha' in globals() else 1.0, random_state=RANDOM_STATE))\n",
    "]\n",
    "\n",
    "meta_learner = Ridge(alpha=1.0, random_state=RANDOM_STATE)\n",
    "\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=meta_learner,\n",
    "    cv=3,\n",
    "    n_jobs=1\n",
    ")\n",
    "\n",
    "stacking_model.fit(X_train_scaled, y_train)\n",
    "\n",
    "stacking_results = evaluate_model(stacking_model, X_train_scaled, y_train, \n",
    "                                  X_val_scaled, y_val, \"Stacking Regressor\")\n",
    "\n",
    "all_models_results.append(stacking_results)\n",
    "trained_models['stacking'] = stacking_model\n",
    "print(\"Stacking Regressor training completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f91cb0",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation & Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffd0f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(all_models_results)\n",
    "results_df = results_df.sort_values('val_rmse')\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Model Performance Comparison (sorted by validation RMSE)\")\n",
    "print(\"=\" * 80)\n",
    "print(results_df.to_string(index=False))\n",
    "\n",
    "best_model_name = results_df.iloc[0]['model_name']\n",
    "best_model_val_rmse = results_df.iloc[0]['val_rmse']\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(f\"Best Model: {best_model_name}\")\n",
    "print(f\"Validation RMSE: {best_model_val_rmse:.4f}\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00806960",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 1, figsize=(12, 10))\n",
    "\n",
    "axes[0].barh(results_df['model_name'], results_df['val_rmse'])\n",
    "axes[0].set_xlabel('Validation RMSE')\n",
    "axes[0].set_title('Model Performance Comparison - Validation RMSE (Lower is Better)')\n",
    "axes[0].invert_yaxis()\n",
    "axes[0].grid(axis='x', alpha=0.3)\n",
    "\n",
    "axes[1].barh(results_df['model_name'], results_df['val_r2'], color='green')\n",
    "axes[1].set_xlabel('Validation R²')\n",
    "axes[1].set_title('Model Performance Comparison - Validation R² (Higher is Better)')\n",
    "axes[1].invert_yaxis()\n",
    "axes[1].grid(axis='x', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "ax.scatter(results_df['train_rmse'], results_df['val_rmse'], s=100, alpha=0.6)\n",
    "\n",
    "for idx, row in results_df.iterrows():\n",
    "    ax.annotate(row['model_name'], \n",
    "                (row['train_rmse'], row['val_rmse']),\n",
    "                xytext=(5, 5), textcoords='offset points',\n",
    "                fontsize=8)\n",
    "\n",
    "min_val = min(results_df['train_rmse'].min(), results_df['val_rmse'].min())\n",
    "max_val = max(results_df['train_rmse'].max(), results_df['val_rmse'].max())\n",
    "ax.plot([min_val, max_val], [min_val, max_val], 'r--', alpha=0.5, label='Ideal Line')\n",
    "\n",
    "ax.set_xlabel('Training RMSE')\n",
    "ax.set_ylabel('Validation RMSE')\n",
    "ax.set_title('Training vs Validation Performance (Detecting Overfitting)')\n",
    "ax.legend()\n",
    "ax.grid(alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ef93a6",
   "metadata": {},
   "source": [
    "## 8. Final Prediction and Submission\n",
    "\n",
    "Using the best model:\n",
    "1. Retrain on the full training set\n",
    "2. Predict on the test set\n",
    "3. Generate a Kaggle submission file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7de4b67",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Retraining Best Model on Full Training Set\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "X_full = pd.concat([X_train_scaled, X_val_scaled])\n",
    "y_full = pd.concat([y_train, y_val])\n",
    "\n",
    "print(f\"Full training set size: {X_full.shape}\")\n",
    "\n",
    "best_model_key = None\n",
    "for key, results in zip(trained_models.keys(), all_models_results):\n",
    "    if results['model_name'] == best_model_name:\n",
    "        best_model_key = key\n",
    "        break\n",
    "\n",
    "if best_model_key == 'stacking':\n",
    "    final_model = StackingRegressor(\n",
    "        estimators=base_learners,\n",
    "        final_estimator=meta_learner,\n",
    "        cv=5\n",
    "    )\n",
    "elif best_model_key == 'random_forest':\n",
    "    final_model = rf_model\n",
    "elif best_model_key == 'gradient_boosting':\n",
    "    final_model = gb_model\n",
    "else:\n",
    "    final_model = trained_models[best_model_key]\n",
    "\n",
    "print(f\"Retraining model: {best_model_name}\")\n",
    "final_model.fit(X_full, y_full)\n",
    "print(\"Training completed\")\n",
    "\n",
    "model_filename = MODEL_DIR / f'final_model_{best_model_key}.pkl'\n",
    "joblib.dump(final_model, model_filename)\n",
    "print(f\"Model saved to: {model_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368d17cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Generating Predictions on Test Set\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_predictions = final_model.predict(X_test_scaled)\n",
    "\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission = pd.DataFrame({\n",
    "    id_col: test_ids,\n",
    "    target_col: test_predictions\n",
    "})\n",
    "\n",
    "submission_filename = SUBMISSION_DIR / f'submission_{best_model_key}_{timestamp}.csv'\n",
    "submission.to_csv(submission_filename, index=False)\n",
    "\n",
    "print(f\"Submission file generated!\")\n",
    "print(f\"File path: {submission_filename}\")\n",
    "print(f\"\\nPrediction Statistics:\")\n",
    "print(f\"  Mean: {test_predictions.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(test_predictions):.4f}\")\n",
    "print(f\"  Standard Deviation: {test_predictions.std():.4f}\")\n",
    "print(f\"  Minimum: {test_predictions.min():.4f}\")\n",
    "print(f\"  Maximum: {test_predictions.max():.4f}\")\n",
    "\n",
    "print(f\"\\nFirst 10 rows of the submission file:\")\n",
    "print(submission.head(10).to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002fee61",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv(MODEL_DIR / 'model_comparison.csv', index=False)\n",
    "\n",
    "summary = {\n",
    "    'project_name': 'Kaggle Regression Project',\n",
    "    'completion_date': datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "    'best_model': best_model_name,\n",
    "    'best_val_rmse': float(best_model_val_rmse),\n",
    "    'n_features_original': len(numeric_features) + len(categorical_features),\n",
    "    'n_features_engineered': X_full.shape[1],\n",
    "    'n_models_trained': len(all_models_results),\n",
    "    'submission_file': str(submission_filename)\n",
    "}\n",
    "\n",
    "import json\n",
    "with open(MODEL_DIR / 'project_summary.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, indent=2, ensure_ascii=False)\n",
    "print(json.dumps(summary, indent=2, ensure_ascii=False))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
