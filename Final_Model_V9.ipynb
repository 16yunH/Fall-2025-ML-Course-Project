{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a5bb9e11",
   "metadata": {},
   "source": [
    "## Part 1: Data Loading and Exploration\n",
    "\n",
    "### 1.1 Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f2bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import StackingRegressor, ExtraTreesRegressor\n",
    "\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e7151d0",
   "metadata": {},
   "source": [
    "### 1.2 Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81360a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path('../data/raw')\n",
    "SUBMISSION_DIR = Path('../submissions')\n",
    "SUBMISSION_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train = pd.read_csv(DATA_DIR / 'cattle_data_train.csv')\n",
    "test = pd.read_csv(DATA_DIR / 'cattle_data_test.csv')\n",
    "\n",
    "print(f\"Training data shape: {train.shape}\")\n",
    "print(f\"Test data shape: {test.shape}\")\n",
    "\n",
    "ID_COL = 'Cattle_ID'\n",
    "TARGET_COL = 'Milk_Yield_L'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad892077",
   "metadata": {},
   "source": [
    "### 1.3 Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a921539",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== Training Data Info ===\")\n",
    "print(train.info())\n",
    "\n",
    "print(\"\\n=== First Few Rows ===\")\n",
    "display(train.head())\n",
    "\n",
    "print(\"\\n=== Target Variable Statistics ===\")\n",
    "print(train[TARGET_COL].describe())\n",
    "\n",
    "print(\"\\n=== Missing Values ===\")\n",
    "print(train.isnull().sum()[train.isnull().sum() > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857be583",
   "metadata": {},
   "source": [
    "### 1.4 Separate Features and Target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a988cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_full = train.drop(columns=[ID_COL, TARGET_COL])\n",
    "y = train[TARGET_COL]\n",
    "X_test = test.drop(columns=[ID_COL])\n",
    "test_ids = test[ID_COL]\n",
    "\n",
    "print(f\"\\nTarget stats - Mean: {y.mean():.2f}, Std: {y.std():.2f}\")\n",
    "print(f\"Target range: [{y.min():.2f}, {y.max():.2f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75623971",
   "metadata": {},
   "source": [
    "## Part 2: Feature Engineering\n",
    "\n",
    "### 2.1 Date Feature Engineering\n",
    "\n",
    "Extract temporal and cyclical features from date information to capture seasonality effects on milk production."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb68ce2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date_features(df, date_col):\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    df['date_month'] = df[date_col].dt.month\n",
    "    df['date_quarter'] = df[date_col].dt.quarter\n",
    "    df['date_day_of_week'] = df[date_col].dt.dayofweek\n",
    "    df['date_day_of_year'] = df[date_col].dt.dayofyear\n",
    "    df['date_week_of_year'] = df[date_col].dt.isocalendar().week\n",
    "    df['date_is_weekend'] = (df['date_day_of_week'] >= 5).astype(int)\n",
    "    \n",
    "    df['date_month_sin'] = np.sin(2 * np.pi * df['date_month'] / 12)\n",
    "    df['date_month_cos'] = np.cos(2 * np.pi * df['date_month'] / 12)\n",
    "    df['date_quarter_sin'] = np.sin(2 * np.pi * df['date_quarter'] / 4)\n",
    "    df['date_quarter_cos'] = np.cos(2 * np.pi * df['date_quarter'] / 4)\n",
    "    df['date_week_sin'] = np.sin(2 * np.pi * df['date_week_of_year'] / 52)\n",
    "    df['date_week_cos'] = np.cos(2 * np.pi * df['date_week_of_year'] / 52)\n",
    "    \n",
    "    df['date_season'] = ((df['date_month'] % 12 + 3) // 3) % 4\n",
    "    \n",
    "    df = df.drop(columns=[date_col])\n",
    "    return df\n",
    "\n",
    "X_full = extract_date_features(X_full, 'Date')\n",
    "X_test = extract_date_features(X_test, 'Date')\n",
    "\n",
    "date_features = [col for col in X_full.columns if col.startswith('date_')]\n",
    "print(f\"Created {len(date_features)} date features: {date_features}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d47f700",
   "metadata": {},
   "source": [
    "### 2.2 Farm Statistical Features\n",
    "\n",
    "Create safe statistical features from Farm_ID without target leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e84efcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'Farm_ID' in X_full.columns:\n",
    "    farm_sizes = X_full['Farm_ID'].value_counts().to_dict()\n",
    "    X_full['farm_size'] = X_full['Farm_ID'].map(farm_sizes)\n",
    "    X_test['farm_size'] = X_test['Farm_ID'].map(farm_sizes).fillna(X_full['farm_size'].median())\n",
    "    \n",
    "    X_full['farm_frequency'] = X_full['farm_size'] / len(X_full)\n",
    "    X_test['farm_frequency'] = X_test['farm_size'] / len(X_full)\n",
    "    \n",
    "    farm_rank = X_full['Farm_ID'].value_counts().rank(method='dense', ascending=False).to_dict()\n",
    "    X_full['farm_rank'] = X_full['Farm_ID'].map(farm_rank)\n",
    "    X_test['farm_rank'] = X_test['Farm_ID'].map(farm_rank).fillna(X_full['farm_rank'].median())\n",
    "    \n",
    "    farm_diversity = train.groupby('Farm_ID')[ID_COL].nunique().to_dict()\n",
    "    X_full['farm_diversity'] = X_full['Farm_ID'].map(farm_diversity)\n",
    "    X_test['farm_diversity'] = X_test['Farm_ID'].map(farm_diversity).fillna(X_full['farm_diversity'].median())\n",
    "    \n",
    "    le_farm = LabelEncoder()\n",
    "    all_farms = pd.concat([X_full['Farm_ID'], X_test['Farm_ID']])\n",
    "    le_farm.fit(all_farms.astype(str))\n",
    "    X_full['farm_encoded'] = le_farm.transform(X_full['Farm_ID'].astype(str))\n",
    "    X_test['farm_encoded'] = le_farm.transform(X_test['Farm_ID'].astype(str))\n",
    "    \n",
    "    X_full = X_full.drop(columns=['Farm_ID'])\n",
    "    X_test = X_test.drop(columns=['Farm_ID'])\n",
    "    \n",
    "    print(\"Created 5 farm features: farm_size, farm_frequency, farm_rank, farm_diversity, farm_encoded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af3fa380",
   "metadata": {},
   "source": [
    "### 2.3 Domain-Driven Interaction Features\n",
    "\n",
    "Create interaction features based on dairy science knowledge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c8b80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions_created = 0\n",
    "\n",
    "if 'Age_Months' in X_full.columns and 'Weight_kg' in X_full.columns:\n",
    "    X_full['age_x_weight'] = X_full['Age_Months'] * X_full['Weight_kg']\n",
    "    X_test['age_x_weight'] = X_test['Age_Months'] * X_test['Weight_kg']\n",
    "    X_full['age_weight_ratio'] = X_full['Age_Months'] / (X_full['Weight_kg'] + 1e-5)\n",
    "    X_test['age_weight_ratio'] = X_test['Age_Months'] / (X_test['Weight_kg'] + 1e-5)\n",
    "    interactions_created += 2\n",
    "\n",
    "if 'Parity' in X_full.columns and 'Age_Months' in X_full.columns:\n",
    "    X_full['parity_x_age'] = X_full['Parity'] * X_full['Age_Months']\n",
    "    X_test['parity_x_age'] = X_test['Parity'] * X_test['Age_Months']\n",
    "    interactions_created += 1\n",
    "\n",
    "if 'Temperature_Celsius' in X_full.columns and 'Humidity_Percent' in X_full.columns:\n",
    "    X_full['heat_stress'] = X_full['Temperature_Celsius'] * X_full['Humidity_Percent'] / 100\n",
    "    X_test['heat_stress'] = X_test['Temperature_Celsius'] * X_test['Humidity_Percent'] / 100\n",
    "    X_full['temp_squared'] = X_full['Temperature_Celsius'] ** 2\n",
    "    X_test['temp_squared'] = X_test['Temperature_Celsius'] ** 2\n",
    "    interactions_created += 2\n",
    "\n",
    "if 'Feed_Quantity_kg' in X_full.columns and 'Weight_kg' in X_full.columns:\n",
    "    X_full['feed_per_weight'] = X_full['Feed_Quantity_kg'] / (X_full['Weight_kg'] + 1e-5)\n",
    "    X_test['feed_per_weight'] = X_test['Feed_Quantity_kg'] / (X_test['Weight_kg'] + 1e-5)\n",
    "    interactions_created += 1\n",
    "\n",
    "if 'Feed_Protein_Percent' in X_full.columns and 'Feed_Quantity_kg' in X_full.columns:\n",
    "    X_full['protein_intake'] = X_full['Feed_Protein_Percent'] * X_full['Feed_Quantity_kg'] / 100\n",
    "    X_test['protein_intake'] = X_test['Feed_Protein_Percent'] * X_test['Feed_Quantity_kg'] / 100\n",
    "    interactions_created += 1\n",
    "\n",
    "if 'Feed_Energy_MJ' in X_full.columns and 'Weight_kg' in X_full.columns:\n",
    "    X_full['energy_per_weight'] = X_full['Feed_Energy_MJ'] / (X_full['Weight_kg'] + 1e-5)\n",
    "    X_test['energy_per_weight'] = X_test['Feed_Energy_MJ'] / (X_test['Weight_kg'] + 1e-5)\n",
    "    interactions_created += 1\n",
    "\n",
    "if 'Somatic_Cell_Count' in X_full.columns:\n",
    "    X_full['scc_log'] = np.log1p(X_full['Somatic_Cell_Count'])\n",
    "    X_test['scc_log'] = np.log1p(X_test['Somatic_Cell_Count'])\n",
    "    interactions_created += 1\n",
    "\n",
    "if all(col in X_full.columns for col in ['Parity', 'date_month']):\n",
    "    X_full['lactation_curve'] = X_full['Parity'] * np.exp(-0.05 * X_full['date_month'])\n",
    "    X_test['lactation_curve'] = X_test['Parity'] * np.exp(-0.05 * X_test['date_month'])\n",
    "    interactions_created += 1\n",
    "\n",
    "if all(col in X_full.columns for col in ['Weight_kg', 'Age_Months', 'Feed_Quantity_kg']):\n",
    "    X_full['body_condition'] = X_full['Weight_kg'] / (X_full['Age_Months'] + 1) * X_full['Feed_Quantity_kg']\n",
    "    X_test['body_condition'] = X_test['Weight_kg'] / (X_test['Age_Months'] + 1) * X_test['Feed_Quantity_kg']\n",
    "    interactions_created += 1\n",
    "\n",
    "print(f\"Created {interactions_created} interaction features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee30de4",
   "metadata": {},
   "source": [
    "### 2.4 Polynomial Features\n",
    "\n",
    "Add squared terms for key numerical features to capture non-linear relationships."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d828c16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "key_squared = ['Age_Months', 'Weight_kg', 'Parity', 'Feed_Quantity_kg']\n",
    "squared_count = 0\n",
    "\n",
    "for feat in key_squared:\n",
    "    if feat in X_full.columns:\n",
    "        X_full[f'{feat}_sq'] = X_full[feat] ** 2\n",
    "        X_test[f'{feat}_sq'] = X_test[feat] ** 2\n",
    "        squared_count += 1\n",
    "\n",
    "print(f\"Added {squared_count} squared features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5782f779",
   "metadata": {},
   "source": [
    "## Part 3: Data Preprocessing\n",
    "\n",
    "### 3.1 Encode Categorical Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2025d817",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features = X_full.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "categorical_features = X_full.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "print(f\"Numeric features: {len(numeric_features)}\")\n",
    "print(f\"Categorical features: {len(categorical_features)}\")\n",
    "\n",
    "for col in categorical_features:\n",
    "    le = LabelEncoder()\n",
    "    combined = pd.concat([X_full[col].astype(str), X_test[col].astype(str)])\n",
    "    le.fit(combined)\n",
    "    X_full[col] = le.transform(X_full[col].astype(str))\n",
    "    X_test[col] = le.transform(X_test[col].astype(str))\n",
    "    numeric_features.append(col)\n",
    "\n",
    "print(f\"Encoded {len(categorical_features)} categorical features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb238b3",
   "metadata": {},
   "source": [
    "### 3.2 Handle Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d51636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Missing values before imputation:\")\n",
    "print(X_full.isnull().sum()[X_full.isnull().sum() > 0])\n",
    "\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "X_full[numeric_features] = imputer.fit_transform(X_full[numeric_features])\n",
    "X_test[numeric_features] = imputer.transform(X_test[numeric_features])\n",
    "\n",
    "print(\"\\n Missing values imputed with median strategy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f2f11e",
   "metadata": {},
   "source": [
    "### 3.3 Feature Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0d4934",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_full_scaled = pd.DataFrame(\n",
    "    scaler.fit_transform(X_full),\n",
    "    columns=X_full.columns,\n",
    "    index=X_full.index\n",
    ")\n",
    "X_test_scaled = pd.DataFrame(\n",
    "    scaler.transform(X_test),\n",
    "    columns=X_test.columns,\n",
    "    index=X_test.index\n",
    ")\n",
    "\n",
    "print(f\"Scaled {X_full_scaled.shape[1]} features\")\n",
    "print(f\"\\nFinal feature count: {X_full_scaled.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49874c74",
   "metadata": {},
   "source": [
    "## Part 4: Model Training and Evaluation\n",
    "\n",
    "### 4.1 Cross-Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071e7d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "def evaluate_model(model, X, y, name):\n",
    "    scores = cross_val_score(\n",
    "        model, X, y,\n",
    "        cv=kf,\n",
    "        scoring='neg_root_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    rmse = -scores.mean()\n",
    "    std = scores.std()\n",
    "    print(f\"{name:20s} CV RMSE: {rmse:.4f} (¬±{std:.4f})\")\n",
    "    return rmse\n",
    "\n",
    "print(\"Cross-validation setup: 5-fold with shuffle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6e1838",
   "metadata": {},
   "source": [
    "### 4.2 Train Base Models\n",
    "\n",
    "#### 4.2.1 XGBoost (V7 Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd8fcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training XGBoost with V7's optimized parameters...\")\n",
    "print(\"Key params: lr=0.015, subsample=0.68, reg_alpha=1.5, reg_lambda=3.5\")\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.015,       # Slower learning for better generalization\n",
    "    subsample=0.68,             # Aggressive subsampling to reduce overfitting\n",
    "    colsample_bytree=0.68,\n",
    "    reg_alpha=1.5,              # Strong L1 regularization\n",
    "    reg_lambda=3.5,             # Strong L2 regularization\n",
    "    min_child_weight=6,\n",
    "    gamma=0.12,                 # Minimum loss reduction for split\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    tree_method='hist'\n",
    ")\n",
    "\n",
    "xgb_cv = evaluate_model(xgb_model, X_full_scaled, y, \"XGBoost\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdbd12a0",
   "metadata": {},
   "source": [
    "#### 4.2.2 LightGBM (V7 Configuration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc0aae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining LightGBM with V7's optimized parameters...\")\n",
    "\n",
    "lgb_model = lgb.LGBMRegressor(\n",
    "    n_estimators=800,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.015,\n",
    "    num_leaves=26,\n",
    "    subsample=0.68,\n",
    "    colsample_bytree=0.68,\n",
    "    reg_alpha=1.5,\n",
    "    reg_lambda=3.5,\n",
    "    min_child_weight=6,\n",
    "    min_child_samples=22,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1\n",
    ")\n",
    "\n",
    "lgb_cv = evaluate_model(lgb_model, X_full_scaled, y, \"LightGBM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272c862e",
   "metadata": {},
   "source": [
    "#### 4.2.3 ExtraTrees (For Ensemble Diversity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012e1fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nTraining ExtraTrees for ensemble diversity...\")\n",
    "\n",
    "et_model = ExtraTreesRegressor(\n",
    "    n_estimators=350,\n",
    "    max_depth=14,\n",
    "    min_samples_split=8,\n",
    "    min_samples_leaf=3,\n",
    "    max_features='sqrt',\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "et_cv = evaluate_model(et_model, X_full_scaled, y, \"ExtraTrees\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8866958",
   "metadata": {},
   "source": [
    "### 4.3 Stacking Ensemble\n",
    "\n",
    "Combine base models using a meta-learner (Ridge regression with strong regularization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaad81e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nBuilding Stacking Ensemble...\")\n",
    "print(\"Meta-learner: Ridge regression with alpha=15 (V7's optimal value)\")\n",
    "\n",
    "base_learners = [\n",
    "    ('xgb', xgb_model),\n",
    "    ('lgb', lgb_model),\n",
    "    ('et', et_model)\n",
    "]\n",
    "\n",
    "stacking_model = StackingRegressor(\n",
    "    estimators=base_learners,\n",
    "    final_estimator=Ridge(alpha=15.0),  # Strong regularization in meta-learner\n",
    "    cv=5,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "stack_cv = evaluate_model(stacking_model, X_full_scaled, y, \"Stacking\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1dbdd6",
   "metadata": {},
   "source": [
    "### 4.4 Model Selection and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4549efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {\n",
    "    'XGBoost': (xgb_model, xgb_cv),\n",
    "    'LightGBM': (lgb_model, lgb_cv),\n",
    "    'ExtraTrees': (et_model, et_cv),\n",
    "    'Stacking': (stacking_model, stack_cv)\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL PERFORMANCE SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "for name, (model, cv) in sorted(models.items(), key=lambda x: x[1][1]):\n",
    "    print(f\"{name:20s}: {cv:.4f} RMSE\")\n",
    "\n",
    "best_name = min(models, key=lambda x: models[x][1])\n",
    "best_model, best_cv = models[best_name]\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"üèÜ BEST MODEL: {best_name}\")\n",
    "print(f\"   Cross-Validation RMSE: {best_cv:.4f}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca45309",
   "metadata": {},
   "source": [
    "## Part 5: Final Prediction and Submission\n",
    "\n",
    "### 5.1 Train on Full Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939d95d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nTraining {best_name} on full training dataset...\")\n",
    "best_model.fit(X_full_scaled, y)\n",
    "print(\"Training complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a8e855",
   "metadata": {},
   "source": [
    "### 5.2 Generate Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639afdbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Generating predictions on test set...\")\n",
    "predictions = best_model.predict(X_test_scaled)\n",
    "\n",
    "print(\"\\nPrediction Statistics:\")\n",
    "print(f\"  Mean:   {predictions.mean():.4f}\")\n",
    "print(f\"  Median: {np.median(predictions):.4f}\")\n",
    "print(f\"  Std:    {predictions.std():.4f}\")\n",
    "print(f\"  Min:    {predictions.min():.4f}\")\n",
    "print(f\"  Max:    {predictions.max():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43edc1db",
   "metadata": {},
   "source": [
    "### 5.3 Create Submission File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e32b8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "submission_filename = f'submission_v9_final_{timestamp}.csv'\n",
    "submission_path = SUBMISSION_DIR / submission_filename\n",
    "\n",
    "submission = pd.DataFrame({\n",
    "    ID_COL: test_ids,\n",
    "    TARGET_COL: predictions\n",
    "})\n",
    "\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"\\n Submission file created: {submission_filename}\")\n",
    "print(f\" Path: {submission_path}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nFirst 10 predictions:\")\n",
    "display(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
